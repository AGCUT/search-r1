# âœ… è¯„ä¼°è„šæœ¬ç°åœ¨ä¼šå¯¹æ¯” Search æ¬¡æ•°å’Œç”Ÿæˆæ—¶é—´ï¼

## ğŸ‰ æ›´æ–°å†…å®¹

å¯¹æ¯”è¯„ä¼°è„šæœ¬å·²æ›´æ–°ï¼Œç°åœ¨ä¼šè‡ªåŠ¨ç»Ÿè®¡å¹¶å¯¹æ¯”:

âœ… **è¯„ä¼°æŒ‡æ ‡åˆ†æ•°** (EM, F1, BLEU, ROUGEç­‰)
âœ… **å¹³å‡ Search æ¬¡æ•°** - æ¯ä¸ªé—®é¢˜è°ƒç”¨æ£€ç´¢çš„å¹³å‡æ¬¡æ•°
âœ… **ç”Ÿæˆæ—¶é—´** - æ¨¡å‹æ¨ç†æ—¶é—´å¯¹æ¯”

---

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

è¿è¡Œ `bash configs/qrecc/compare_base_vs_trained_f1.sh` åä¼šçœ‹åˆ°:

```
============================================================================
COMPARISON REPORT
============================================================================

Metric: F1 Score
--------------------------------------------------------------------
Metric                    Base Model           Trained Model        Change
--------------------------------------------------------------------
F1 Score                  0.2340              0.4580              +0.2240 (+95.73%)
Avg Searches/Question     0.08                1.45                +1.37 (1712.5%)
Generation Time (s)       12.34               15.67               +3.33
--------------------------------------------------------------------

âœ“ The trained model shows IMPROVEMENT over the base model

Key Improvements:
  â€¢ F1 Score: +0.2240 (+95.73%)
  â€¢ Search Usage: The trained model makes 1.45 searches/question (vs 0.08 for base)
    â†’ Model learned to use search more actively

============================================================================
Full Results
============================================================================
Base Model Log:    results/qrecc_comparison_f1_20231209/base_model/eval.log
Trained Model Log: results/qrecc_comparison_f1_20231209/trained_model/eval.log

To view detailed logs:
  tail -100 results/qrecc_comparison_f1_20231209/base_model/eval.log
  tail -100 results/qrecc_comparison_f1_20231209/trained_model/eval.log

To extract all metrics:
  grep 'env/' results/qrecc_comparison_f1_20231209/base_model/eval.log
  grep 'env/' results/qrecc_comparison_f1_20231209/trained_model/eval.log
============================================================================
```

---

## ğŸ“ˆ ç»Ÿè®¡çš„æŒ‡æ ‡

### 1. è¯„ä¼°åˆ†æ•°
- æ ¹æ®è®¾ç½®çš„ `REWARD_FN` æ˜¾ç¤ºå¯¹åº”æŒ‡æ ‡
- æ”¯æŒ: `em`, `f1`, `bleu`, `rouge_l`, `rouge_1`, `rouge_2` ç­‰
- æ˜¾ç¤ºç»å¯¹æ”¹è¿›å€¼å’Œç™¾åˆ†æ¯”

### 2. Search æ¬¡æ•°ç»Ÿè®¡

**æŒ‡æ ‡**: `env/number_of_valid_search`

- **å«ä¹‰**: æ¯ä¸ªé—®é¢˜å¹³å‡è°ƒç”¨ `<search>` çš„æ¬¡æ•°
- **æ¥æº**: veRL æ¡†æ¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç»Ÿè®¡
- **è®¡ç®—**: æ‰€æœ‰æ ·æœ¬çš„ search è°ƒç”¨æ¬¡æ•°çš„å¹³å‡å€¼

**è§£è¯»**:
- **åŸºç¡€æ¨¡å‹**: é€šå¸¸å¾ˆå°‘æˆ–ä¸ä½¿ç”¨ search (0.0 - 0.2)
- **è®­ç»ƒåæ¨¡å‹**: åº”è¯¥å­¦ä¼šé€‚å½“ä½¿ç”¨ search (1.0 - 2.0)
- **ç†æƒ³æƒ…å†µ**: max_turns=2 æ—¶,çº¦ 1.0-1.5 searches/question

**ç¤ºä¾‹**:
```
Avg Searches/Question     0.08                1.45                +1.37 (1712.5%)
  â†’ Model learned to use search more actively
```

### 3. ç”Ÿæˆæ—¶é—´

**æŒ‡æ ‡**: `timing/gen`

- **å«ä¹‰**: æ¯ä¸ª batch çš„ç”Ÿæˆæ—¶é—´(ç§’)
- **åŒ…å«**: æ¨¡å‹æ¨ç† + æ£€ç´¢è°ƒç”¨æ—¶é—´
- **æ³¨æ„**: è®­ç»ƒåæ¨¡å‹é€šå¸¸æ›´æ…¢(å› ä¸ºä¼šå®é™…è°ƒç”¨æ£€ç´¢)

---

## ğŸ” æŸ¥çœ‹è¯¦ç»†æŒ‡æ ‡

è„šæœ¬ä¼šæç¤ºä½ å¯ä»¥æŸ¥çœ‹æ›´å¤šæŒ‡æ ‡:

```bash
# æŸ¥çœ‹æ‰€æœ‰ç¯å¢ƒæŒ‡æ ‡
grep 'env/' results/qrecc_comparison_*/base_model/eval.log
grep 'env/' results/qrecc_comparison_*/trained_model/eval.log
```

**å¯ç”¨çš„ `env/` æŒ‡æ ‡**:

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| `env/number_of_valid_search` | å¹³å‡ search æ¬¡æ•° |
| `env/ratio_of_valid_action` | æœ‰æ•ˆactionæ¯”ä¾‹ |
| `env/number_of_valid_action` | å¹³å‡æœ‰æ•ˆactionæ•° |
| `env/finish_ratio` | å®Œæˆæ¯”ä¾‹ |

---

## ğŸ’¡ ç»“æœåˆ†æ

### Search æ¬¡æ•°åˆ†æ

**åœºæ™¯ 1: Search æ¬¡æ•°æ˜¾è‘—å¢åŠ **
```
Avg Searches/Question: 0.05 â†’ 1.45 (+1.40)
```
âœ… **å¥½ç°è±¡** - æ¨¡å‹å­¦ä¼šäº†ä½¿ç”¨æ£€ç´¢å·¥å…·
â†’ è¯´æ˜ RL è®­ç»ƒæˆåŠŸè®©æ¨¡å‹å­¦ä¼šè°ƒç”¨ search

**åœºæ™¯ 2: Search æ¬¡æ•°ç•¥å¾®å‡å°‘**
```
Avg Searches/Question: 1.80 â†’ 1.35 (-0.45)
```
âœ… **å¯èƒ½ä¹Ÿæ˜¯å¥½ç°è±¡** - æ¨¡å‹å˜å¾—æ›´æœ‰é€‰æ‹©æ€§
â†’ å¦‚æœåŒæ—¶ F1/ROUGE åˆ†æ•°æå‡,è¯´æ˜æ¨¡å‹å­¦ä¼šäº†æ›´é«˜æ•ˆåœ°ä½¿ç”¨ search

**åœºæ™¯ 3: Search æ¬¡æ•°æ²¡æœ‰å˜åŒ–**
```
Avg Searches/Question: 0.02 â†’ 0.03 (+0.01)
```
âš ï¸ **éœ€è¦æ³¨æ„** - æ¨¡å‹å¯èƒ½æ²¡æœ‰å­¦ä¼šä½¿ç”¨ search
â†’ æ£€æŸ¥è®­ç»ƒé…ç½®ã€prompt æ ¼å¼ã€retriever è¿æ¥

### ç”Ÿæˆæ—¶é—´åˆ†æ

**æ­£å¸¸æƒ…å†µ**:
```
Generation Time: 10.5s â†’ 14.2s (+3.7s)
```
- è®­ç»ƒåæ¨¡å‹æ›´æ…¢æ˜¯æ­£å¸¸çš„(å› ä¸ºå®é™…è°ƒç”¨æ£€ç´¢)
- æ¯æ¬¡ search è°ƒç”¨çº¦å¢åŠ  1-3 ç§’

**å¼‚å¸¸æƒ…å†µ**:
```
Generation Time: 10.5s â†’ 25.8s (+15.3s)
```
âš ï¸ å¯èƒ½çš„é—®é¢˜:
- æ£€ç´¢æœåŠ¡å™¨å“åº”æ…¢
- æ¨¡å‹ç”Ÿæˆtokenæ•°è¿‡å¤š
- ç½‘ç»œå»¶è¿Ÿ

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### è¯„ä¼°æµç¨‹

```bash
# 1. å¯åŠ¨æ£€ç´¢æœåŠ¡å™¨
bash retrieval_launch.sh

# 2. ç¡®è®¤æ£€ç´¢æœåŠ¡å™¨å·¥ä½œ
curl -X POST http://127.0.0.1:8000/retrieve \
    -H "Content-Type: application/json" \
    -d '{"queries": ["test"], "topk": 3}'

# 3. è¿è¡Œå¯¹æ¯”è¯„ä¼°
bash configs/qrecc/compare_base_vs_trained_f1.sh

# 4. æŸ¥çœ‹å®Œæ•´ç»Ÿè®¡
grep 'env/' results/qrecc_comparison_*/base_model/eval.log
grep 'env/' results/qrecc_comparison_*/trained_model/eval.log
```

### å¯¹æ¯”ä¸åŒæŒ‡æ ‡çš„ Search è¡Œä¸º

```bash
#!/bin/bash
# å¯¹æ¯”ä¸åŒæŒ‡æ ‡ä¸‹çš„ search è¡Œä¸º

for METRIC in f1 rouge_l bleu; do
    echo "========== Testing with $METRIC =========="
    export REWARD_FN=$METRIC
    bash configs/qrecc/compare_base_vs_trained_f1.sh

    # æå– search ç»Ÿè®¡
    echo ""
    echo "Search statistics for $METRIC:"
    grep "number_of_valid_search" results/qrecc_comparison_${METRIC}_*/*/eval.log
    echo ""
done
```

---

## ğŸ“ æ›´æ–°çš„æ–‡ä»¶

- **`configs/qrecc/compare_base_vs_trained_f1.sh`** âœ¨
  - âœ… æ·»åŠ  search æ¬¡æ•°ç»Ÿè®¡
  - âœ… æ·»åŠ ç”Ÿæˆæ—¶é—´ç»Ÿè®¡
  - âœ… æ›´å¥½çš„è¡¨æ ¼æ ¼å¼è¾“å‡º
  - âœ… æ™ºèƒ½åˆ†æ(search ä½¿ç”¨æ¨¡å¼)

---

## ğŸ”§ è‡ªå®šä¹‰ç»Ÿè®¡

å¦‚æœä½ æƒ³æå–å…¶ä»–æŒ‡æ ‡,å¯ä»¥åœ¨è„šæœ¬æœ«å°¾æ·»åŠ :

```bash
# æå–å…¶ä»–æŒ‡æ ‡
BASE_ACTION=$(grep -oP "(?<=env/number_of_valid_action:\s)\d+\.\d+" "$OUTPUT_BASE_DIR/base_model/eval.log" | tail -1)
TRAINED_ACTION=$(grep -oP "(?<=env/number_of_valid_action:\s)\d+\.\d+" "$OUTPUT_BASE_DIR/trained_model/eval.log" | tail -1)

echo "Valid Actions: $BASE_ACTION â†’ $TRAINED_ACTION"
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹è¾“å‡º

```
============================================================================
QReCC Model Comparison - Base vs Trained
============================================================================
Base Model:         Qwen/Qwen2.5-3B
Trained Checkpoint: /usr/yuque/guo/searchr1/verl_checkpoints/.../global_step_200
Data Directory:     data/qrecc_raw
Evaluation Metric:  rouge_l
Output Directory:   results/qrecc_comparison_rouge_l_20231209_143022
GPUs:               0,1,2,3,4,5,6,7
============================================================================

[... è¯„ä¼°è¿‡ç¨‹ ...]

============================================================================
COMPARISON REPORT
============================================================================

Metric: ROUGE_L Score
--------------------------------------------------------------------
Metric                    Base Model           Trained Model        Change
--------------------------------------------------------------------
ROUGE_L Score             0.3245              0.5782              +0.2537 (+78.18%)
Avg Searches/Question     0.05                1.52                +1.47 (2940.0%)
Generation Time (s)       11.23               16.87               +5.64
--------------------------------------------------------------------

âœ“ The trained model shows IMPROVEMENT over the base model

Key Improvements:
  â€¢ ROUGE_L Score: +0.2537 (+78.18%)
  â€¢ Search Usage: The trained model makes 1.52 searches/question (vs 0.05 for base)
    â†’ Model learned to use search more actively
```

---

## æ€»ç»“

ç°åœ¨è¯„ä¼°è„šæœ¬ä¼šå®Œæ•´åœ°å¯¹æ¯”:

âœ… **å‡†ç¡®æ€§**: EM / F1 / BLEU / ROUGE ç­‰åˆ†æ•°
âœ… **Search è¡Œä¸º**: è°ƒç”¨æ£€ç´¢çš„é¢‘ç‡
âœ… **æ•ˆç‡**: ç”Ÿæˆæ—¶é—´å¯¹æ¯”

è¿™è®©ä½ èƒ½å¤Ÿå…¨é¢äº†è§£æ¨¡å‹åœ¨ RL è®­ç»ƒåçš„æ”¹è¿›æƒ…å†µï¼ğŸš€