# æœªæ¥æ”¹è¿›æ–¹æ¡ˆ

æœ¬æ–‡æ¡£è®°å½•äº†é’ˆå¯¹å½“å‰ç³»ç»Ÿçš„ä¼˜åŒ–å»ºè®®å’Œå…·ä½“å®æ–½æ–¹æ³•ã€‚

## ğŸ“Š å½“å‰é—®é¢˜åˆ†æ

### ä¸»è¦é—®é¢˜
**ç›¸å¯¹ä½ç½®å…³ç³»ç±»é—®é¢˜å›ç­”ä¸å‡†ç¡®**

**é—®é¢˜è¡¨ç°**:
- å¯¹äº"éƒ¨ä»¶Aåœ¨éƒ¨ä»¶Bçš„å“ªä¸ªæ–¹å‘"è¿™ç±»é—®é¢˜ï¼Œå‡†ç¡®ç‡ä¸å¤Ÿé«˜
- æ¨¡å‹éš¾ä»¥å‡†ç¡®ç†è§£å¤æ‚çš„ç©ºé—´å…³ç³»

**æ ¹æœ¬åŸå› **:
1. æ•´é¡µå›¾åƒåŒ…å«è¿‡å¤šä¿¡æ¯ï¼Œæ¨¡å‹æ³¨æ„åŠ›åˆ†æ•£
2. å›¾åƒåˆ†è¾¨ç‡å—é™ï¼ˆMAX_PIXELSé™åˆ¶ï¼‰
3. ç¼ºä¹æ˜¾å¼çš„æ¨ç†æ­¥éª¤è®­ç»ƒ
4. ç©ºé—´å…³ç³»æ¨ç†éœ€è¦å¤šæ­¥éª¤æ€è€ƒ

---

## ğŸ¯ æ”¹è¿›æ–¹æ¡ˆ

## æ–¹æ¡ˆ1: æ–‡æ¡£ç»“æ„åŒ– + ç²¾ç¡®å›¾ç‰‡å®šä½

### ğŸ“‹ ç›®æ ‡
- å¯¹äº"é—®æŸé¡µæŸå›¾"çš„é—®é¢˜ï¼Œåªè®©æ¨¡å‹çœ‹åˆ°ç²¾ç¡®è£å‰ªçš„å›¾ç‰‡
- æé«˜å•å›¾åˆ†è¾¨ç‡ï¼Œä½¿ç”¨æ›´é«˜çš„MAX_PIXELS
- å‡å°‘æ— å…³ä¿¡æ¯å¹²æ‰°

### ğŸ’¡ æ ¸å¿ƒæ€è·¯
```
åŸæ–¹æ¡ˆ: é—®é¢˜ â†’ æ£€ç´¢æ•´é¡µ â†’ æ•´é¡µå›¾åƒ(ä½åˆ†è¾¨ç‡) â†’ æ¨¡å‹
æ–°æ–¹æ¡ˆ: é—®é¢˜ â†’ è§£æå›¾å· â†’ ç²¾ç¡®è£å‰ªå›¾ç‰‡ â†’ å•å›¾(é«˜åˆ†è¾¨ç‡) â†’ æ¨¡å‹
```

### ğŸ”§ å…·ä½“å®æ–½æ­¥éª¤

#### æ­¥éª¤1: æ–‡æ¡£ç»“æ„åŒ–è§£æ

**1.1 ç‰ˆé¢åˆ†æ**
```python
# æ–°å»ºæ–‡ä»¶: ccks2025_pdf_multimodal/round_b/document_structure_parser.py

import fitz  # PyMuPDF
from PIL import Image
import numpy as np
import cv2

class DocumentStructureParser:
    """æ–‡æ¡£ç»“æ„åŒ–è§£æå™¨"""

    def __init__(self):
        self.figure_detector = self.load_figure_detector()

    def parse_page_structure(self, pdf_path, page_num):
        """
        è§£æå•é¡µçš„ç»“æ„ï¼Œè¯†åˆ«å›¾ç‰‡åŒºåŸŸ

        Returns:
            {
                'page_num': int,
                'figures': [
                    {
                        'figure_id': str,  # å¦‚ "å›¾1", "å›¾2"
                        'bbox': (x0, y0, x1, y1),  # è¾¹ç•Œæ¡†
                        'confidence': float
                    }
                ],
                'text_regions': [...],
                'tables': [...]
            }
        """
        doc = fitz.open(pdf_path)
        page = doc.load_page(page_num - 1)

        # æ–¹æ³•1: ä½¿ç”¨PyMuPDFçš„å›¾åƒæ£€æµ‹
        figures = self._detect_figures_pymupdf(page)

        # æ–¹æ³•2: ä½¿ç”¨OCRè¯†åˆ«å›¾å·
        figures = self._enhance_with_ocr(page, figures)

        # æ–¹æ³•3: ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        # figures = self._enhance_with_dl_model(page, figures)

        return {
            'page_num': page_num,
            'figures': figures
        }

    def _detect_figures_pymupdf(self, page):
        """ä½¿ç”¨PyMuPDFæ£€æµ‹å›¾ç‰‡"""
        figures = []
        image_list = page.get_images()

        for img_index, img in enumerate(image_list):
            xref = img[0]
            bbox = page.get_image_bbox(img)

            figures.append({
                'figure_id': f'å›¾{img_index + 1}',
                'bbox': bbox,
                'xref': xref,
                'confidence': 1.0
            })

        return figures

    def _enhance_with_ocr(self, page, figures):
        """ä½¿ç”¨OCRè¯†åˆ«å›¾å·ï¼Œæé«˜å‡†ç¡®æ€§"""
        from qwen_vl_utils import process_vision_info

        # è½¬æ¢é¡µé¢ä¸ºå›¾åƒ
        pix = page.get_pixmap(dpi=300)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

        # ä½¿ç”¨OCRè¯†åˆ«æ–‡æœ¬
        ocr_results = self._run_ocr(img)

        # åŒ¹é…å›¾å·ï¼ˆå¦‚"å›¾1"ã€"Fig.1"ï¼‰
        import re
        pattern = r'å›¾\s*(\d+)|Fig\.\s*(\d+)'

        for ocr_result in ocr_results:
            text = ocr_result['text']
            match = re.search(pattern, text)
            if match:
                fig_num = match.group(1) or match.group(2)
                # æ›´æ–°å¯¹åº”å›¾ç‰‡çš„ä¿¡æ¯
                self._update_figure_id(figures, ocr_result['bbox'], f'å›¾{fig_num}')

        return figures

    def extract_figure(self, pdf_path, page_num, figure_id, output_path):
        """
        ç²¾ç¡®æå–æŸä¸ªå›¾ç‰‡

        Args:
            pdf_path: PDFè·¯å¾„
            page_num: é¡µç 
            figure_id: å›¾ç‰‡IDï¼ˆå¦‚"å›¾1"ï¼‰
            output_path: è¾“å‡ºè·¯å¾„
        """
        structure = self.parse_page_structure(pdf_path, page_num)

        # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡
        target_figure = None
        for fig in structure['figures']:
            if fig['figure_id'] == figure_id:
                target_figure = fig
                break

        if not target_figure:
            raise ValueError(f"æœªæ‰¾åˆ° {figure_id}")

        # æå–å›¾ç‰‡
        doc = fitz.open(pdf_path)
        page = doc.load_page(page_num - 1)

        # è£å‰ªå›¾ç‰‡åŒºåŸŸï¼ˆæ·»åŠ paddingï¼‰
        bbox = target_figure['bbox']
        padding = 20  # æ·»åŠ 20åƒç´ padding
        clip_rect = fitz.Rect(
            bbox[0] - padding,
            bbox[1] - padding,
            bbox[2] + padding,
            bbox[3] + padding
        )

        # é«˜åˆ†è¾¨ç‡æ¸²æŸ“
        pix = page.get_pixmap(clip=clip_rect, dpi=600)
        pix.save(output_path)

        return output_path
```

**1.2 æ‰¹é‡è§£æå’Œç¼“å­˜**
```python
# æ–°å»ºæ–‡ä»¶: ccks2025_pdf_multimodal/round_b/batch_structure_parser.py

import json
from pathlib import Path
from tqdm import tqdm
import pandas as pd

def batch_parse_documents(pdf_dir, output_dir):
    """
    æ‰¹é‡è§£ææ‰€æœ‰PDFçš„ç»“æ„

    Args:
        pdf_dir: PDFç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    """
    parser = DocumentStructureParser()
    pdf_files = list(Path(pdf_dir).glob('*.pdf'))

    structures = {}

    for pdf_file in tqdm(pdf_files, desc="è§£ææ–‡æ¡£ç»“æ„"):
        doc_name = pdf_file.stem
        doc = fitz.open(str(pdf_file))

        page_structures = []
        for page_num in range(1, doc.page_count + 1):
            structure = parser.parse_page_structure(str(pdf_file), page_num)
            page_structures.append(structure)

        structures[doc_name] = page_structures

        # ä¿å­˜åˆ°JSON
        output_file = Path(output_dir) / f'{doc_name}_structure.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(page_structures, f, ensure_ascii=False, indent=2)

    return structures

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # è§£æè®­ç»ƒé›†
    batch_parse_documents(
        pdf_dir='/data/coding/patent_b/train/documents',
        output_dir='/data/coding/patent_b/train/structures'
    )

    # è§£ææµ‹è¯•é›†
    batch_parse_documents(
        pdf_dir='/data/coding/patent_b/test/documents',
        output_dir='/data/coding/patent_b/test/structures'
    )
```

#### æ­¥éª¤2: é—®é¢˜è§£æå’Œå›¾ç‰‡å®šä½

**2.1 é—®é¢˜è§£æå™¨**
```python
# åœ¨ test_b_style_refer_215.py ä¸­æ·»åŠ 

import re
from typing import Optional, Tuple

def parse_question_figure_reference(question: str) -> Optional[Tuple[int, str]]:
    """
    è§£æé—®é¢˜ä¸­çš„å›¾ç‰‡å¼•ç”¨

    Args:
        question: é—®é¢˜æ–‡æœ¬

    Returns:
        (page_num, figure_id) æˆ– None

    Examples:
        "è§‚å¯Ÿæ–‡ä»¶ä¸­ç¬¬6é¡µçš„å›¾1ï¼Œç¼–å·ä¸º12çš„éƒ¨ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ" â†’ (6, "å›¾1")
        "ç¬¬3é¡µå›¾2ä¸­ï¼Œéƒ¨ä»¶Aåœ¨éƒ¨ä»¶Bçš„å“ªä¸ªä½ç½®ï¼Ÿ" â†’ (3, "å›¾2")
    """
    # æ¨¡å¼1: "ç¬¬Xé¡µçš„å›¾Y"
    pattern1 = r'ç¬¬\s*(\d+)\s*é¡µ.*?å›¾\s*(\d+)'
    match = re.search(pattern1, question)
    if match:
        page_num = int(match.group(1))
        fig_num = match.group(2)
        return (page_num, f'å›¾{fig_num}')

    # æ¨¡å¼2: "ç¬¬Xé¡µ" (æ²¡æœ‰æŒ‡å®šå›¾å·)
    pattern2 = r'ç¬¬\s*(\d+)\s*é¡µ'
    match = re.search(pattern2, question)
    if match:
        page_num = int(match.group(1))
        return (page_num, None)  # Noneè¡¨ç¤ºæ•´é¡µ

    return None

def should_use_figure_crop(question: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å›¾ç‰‡è£å‰ª"""
    # å…³é”®è¯ï¼šç›¸å¯¹ä½ç½®ã€æ–¹å‘ã€ç©ºé—´å…³ç³»
    position_keywords = ['ä½ç½®', 'æ–¹å‘', 'ä¸Šæ–¹', 'ä¸‹æ–¹', 'å·¦ä¾§', 'å³ä¾§', 'æ—è¾¹', 'ä¹‹é—´']
    return any(keyword in question for keyword in position_keywords)
```

**2.2 ä¿®æ”¹æ¨ç†è„šæœ¬**
```python
# ä¿®æ”¹ test_b_style_refer_215.py

def get_optimized_image_input(question, document_name, question_idx):
    """
    æ ¹æ®é—®é¢˜æ™ºèƒ½é€‰æ‹©è¾“å…¥å›¾åƒ

    Returns:
        {
            'images': [å›¾åƒè·¯å¾„åˆ—è¡¨],
            'max_pixels': å»ºè®®çš„MAX_PIXELSå€¼,
            'mode': 'full_page' æˆ– 'cropped_figure'
        }
    """
    # è§£æé—®é¢˜
    fig_ref = parse_question_figure_reference(question)

    if fig_ref and should_use_figure_crop(question):
        page_num, figure_id = fig_ref

        # åŠ è½½æ–‡æ¡£ç»“æ„
        structure_file = f'/data/coding/patent_b/test/structures/{document_name}_structure.json'
        with open(structure_file, 'r') as f:
            structures = json.load(f)

        page_structure = structures[page_num - 1]

        # å¦‚æœæŒ‡å®šäº†å›¾å·
        if figure_id:
            # æå–å•ä¸ªå›¾ç‰‡
            parser = DocumentStructureParser()
            cropped_path = f'/tmp/{document_name}_p{page_num}_{figure_id}.jpg'
            parser.extract_figure(
                pdf_path=f'/data/coding/patent_b/test/documents/{document_name}.pdf',
                page_num=page_num,
                figure_id=figure_id,
                output_path=cropped_path
            )

            return {
                'images': [cropped_path],
                'max_pixels': 2352000,  # æ›´é«˜åˆ†è¾¨ç‡
                'mode': 'cropped_figure'
            }

    # é»˜è®¤æ–¹æ¡ˆï¼šä½¿ç”¨æ•´é¡µ
    similar_pages = get_similar_image_embedding(document_name, question_idx, top_k=2)
    image_paths = [f'/data/coding/patent_b/test/pdf_img/{document_name}/{p}.jpg'
                   for p in similar_pages]

    return {
        'images': image_paths,
        'max_pixels': 1568000,
        'mode': 'full_page'
    }
```

#### æ­¥éª¤3: æ›´æ–°é¢„å¤„ç†è„šæœ¬

```bash
# æ–°å»ºè„šæœ¬: scripts/01_preprocess_enhanced.sh

#!/bin/bash
# å¢å¼ºç‰ˆé¢„å¤„ç†ï¼šåŒ…å«æ–‡æ¡£ç»“æ„åŒ–

set -e

echo "=========================================="
echo "å¢å¼ºç‰ˆæ•°æ®é¢„å¤„ç†"
echo "=========================================="

# æ­¥éª¤1: åŸæœ‰é¢„å¤„ç†
bash scripts/01_preprocess.sh

# æ­¥éª¤2: æ–‡æ¡£ç»“æ„åŒ–è§£æ
cd ccks2025_pdf_multimodal/round_b

echo "æ­¥éª¤2: è§£ææ–‡æ¡£ç»“æ„..."
python batch_structure_parser.py

echo "æ–‡æ¡£ç»“æ„è§£æå®Œæˆï¼"
echo "ç»“æ„æ–‡ä»¶ä¿å­˜åœ¨: /data/coding/patent_b/{train,test}/structures/"
```

### ğŸ“Š é¢„æœŸæ•ˆæœ

**åˆ†è¾¨ç‡æå‡**:
- åŸæ–¹æ¡ˆ: æ•´é¡µ 1568000 pixels (~2000 tokens)
- æ–°æ–¹æ¡ˆ: å•å›¾ 2352000+ pixels (~3000+ tokens)
- **æå‡**: ~50%+

**å‡†ç¡®ç‡æå‡**:
- é¢„è®¡ä½ç½®å…³ç³»é—®é¢˜å‡†ç¡®ç‡æå‡ **10-15%**

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å›¾å·è¯†åˆ«å‡†ç¡®æ€§**: éœ€è¦å‡†ç¡®è¯†åˆ«å›¾å·ï¼Œå¯èƒ½éœ€è¦å¤šç§æ–¹æ³•ç»“åˆ
2. **è¾¹ç•Œæ¡†ç²¾ç¡®æ€§**: è£å‰ªæ—¶éœ€è¦åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡
3. **ç¼“å­˜ç®¡ç†**: è£å‰ªçš„å›¾ç‰‡éœ€è¦åˆç†ç¼“å­˜
4. **å¤±è´¥å›é€€**: å¦‚æœæ— æ³•è£å‰ªï¼Œåº”è¯¥å›é€€åˆ°æ•´é¡µæ–¹æ¡ˆ

---

## æ–¹æ¡ˆ2: æ•°æ®å¢å¼º - æ¨ç†é“¾ç”Ÿæˆ

### ğŸ“‹ ç›®æ ‡
- ä¸ºè®­ç»ƒæ•°æ®æ·»åŠ æ¨ç†é“¾ï¼ˆChain-of-Thoughtï¼‰
- æ•™ä¼šæ¨¡å‹ä¸€æ­¥æ­¥æ¨ç†

### ğŸ’¡ æ ¸å¿ƒæ€è·¯
```
åŸè®­ç»ƒæ•°æ®: é—®é¢˜ â†’ ç­”æ¡ˆ
æ–°è®­ç»ƒæ•°æ®: é—®é¢˜ â†’ æ¨ç†æ­¥éª¤ â†’ ç­”æ¡ˆ
```

### ğŸ”§ å…·ä½“å®æ–½æ­¥éª¤

#### æ­¥éª¤1: ä½¿ç”¨å¼ºæ¨¡å‹ç”Ÿæˆæ¨ç†é“¾

**1.1 æ¨ç†é“¾ç”Ÿæˆå™¨**
```python
# æ–°å»ºæ–‡ä»¶: ccks2025_pdf_multimodal/round_b/reasoning_chain_generator.py

from vllm import LLM, SamplingParams
import json

class ReasoningChainGenerator:
    """æ¨ç†é“¾ç”Ÿæˆå™¨"""

    def __init__(self, model_path):
        # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ï¼ˆå¦‚Qwen2.5-VL-72Bæˆ–å¾®è°ƒåçš„æ¨¡å‹ï¼‰
        self.model = LLM(model=model_path, tensor_parallel_size=8)

    def generate_reasoning_chain(self, question, images, ground_truth_answer):
        """
        ç”Ÿæˆæ¨ç†é“¾

        Args:
            question: é—®é¢˜
            images: å›¾åƒè·¯å¾„åˆ—è¡¨
            ground_truth_answer: æ­£ç¡®ç­”æ¡ˆï¼ˆç”¨äºéªŒè¯ï¼‰

        Returns:
            {
                'reasoning_steps': [æ­¥éª¤1, æ­¥éª¤2, ...],
                'final_answer': ç­”æ¡ˆ,
                'confidence': ç½®ä¿¡åº¦
            }
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“åˆ©åˆ†æä¸“å®¶ã€‚è¯·ä¸€æ­¥æ­¥åˆ†æä¸‹é¢çš„é—®é¢˜ï¼Œè¯¦ç»†è¯´æ˜æ¨ç†è¿‡ç¨‹ã€‚

é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

ã€åˆ†ææ­¥éª¤ã€‘
1. é¦–å…ˆï¼Œæˆ‘éœ€è¦è¯†åˆ«å›¾ä¸­çš„å…³é”®ä¿¡æ¯...
2. ç„¶åï¼Œæˆ‘éœ€è¦ç¡®å®šéƒ¨ä»¶çš„ä½ç½®å…³ç³»...
3. æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦...
4. æœ€åï¼Œæ ¹æ®ä»¥ä¸Šåˆ†æ...

ã€æœ€ç»ˆç­”æ¡ˆã€‘
{ground_truth_answer}

ç°åœ¨å¼€å§‹ä½ çš„åˆ†æï¼š"""

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    *[{"type": "image", "image": img} for img in images]
                ]
            }
        ]

        sampling_params = SamplingParams(
            temperature=0.7,  # ç¨é«˜çš„æ¸©åº¦è·å¾—å¤šæ ·æ€§
            max_tokens=1024,
            top_p=0.9
        )

        response = self.model.generate(messages, sampling_params)
        reasoning_text = response[0].outputs[0].text

        # è§£ææ¨ç†æ­¥éª¤
        steps = self._parse_reasoning_steps(reasoning_text)

        return {
            'reasoning_steps': steps,
            'reasoning_text': reasoning_text,
            'ground_truth': ground_truth_answer
        }

    def _parse_reasoning_steps(self, text):
        """è§£ææ¨ç†æ­¥éª¤"""
        import re

        # æå–ã€åˆ†ææ­¥éª¤ã€‘éƒ¨åˆ†
        steps_match = re.search(r'ã€åˆ†ææ­¥éª¤ã€‘\n(.*?)\nã€æœ€ç»ˆç­”æ¡ˆã€‘', text, re.DOTALL)
        if not steps_match:
            return []

        steps_text = steps_match.group(1)
        steps = re.findall(r'\d+\.\s*(.*?)(?=\n\d+\.|\Z)', steps_text, re.DOTALL)

        return [step.strip() for step in steps]
```

**1.2 æ‰¹é‡ç”Ÿæˆæ¨ç†é“¾**
```python
# æ–°å»ºæ–‡ä»¶: ccks2025_pdf_multimodal/round_b/batch_generate_reasoning.py

import pandas as pd
from tqdm import tqdm
import json

def batch_generate_reasoning_chains(
    train_data_path,
    output_path,
    sample_rate=0.3  # å¯¹30%çš„æ•°æ®ç”Ÿæˆæ¨ç†é“¾
):
    """
    æ‰¹é‡ä¸ºè®­ç»ƒæ•°æ®ç”Ÿæˆæ¨ç†é“¾

    Args:
        train_data_path: åŸå§‹è®­ç»ƒæ•°æ®è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        sample_rate: é‡‡æ ·ç‡ï¼ˆé‡ç‚¹é‡‡æ ·ä½ç½®å…³ç³»ç±»é—®é¢˜ï¼‰
    """
    generator = ReasoningChainGenerator(
        model_path='/data/coding/llm_model/Qwen/Qwen2___5-VL-32B-Instruct'
    )

    # åŠ è½½è®­ç»ƒæ•°æ®
    df = pd.read_json(train_data_path, lines=True)

    # ç­›é€‰ä½ç½®å…³ç³»ç±»é—®é¢˜ï¼ˆä¼˜å…ˆç”Ÿæˆï¼‰
    position_keywords = ['ä½ç½®', 'æ–¹å‘', 'ä¸Šæ–¹', 'ä¸‹æ–¹', 'å·¦ä¾§', 'å³ä¾§']

    def is_position_question(q):
        return any(kw in q for kw in position_keywords)

    df['is_position'] = df['question'].apply(is_position_question)

    # é‡‡æ ·ç­–ç•¥ï¼šä½ç½®ç±»é—®é¢˜å…¨éƒ¨ç”Ÿæˆï¼Œå…¶ä»–é—®é¢˜éƒ¨åˆ†ç”Ÿæˆ
    position_samples = df[df['is_position']]
    other_samples = df[~df['is_position']].sample(
        n=int(len(df) * sample_rate),
        random_state=42
    )

    samples_to_process = pd.concat([position_samples, other_samples])

    # ç”Ÿæˆæ¨ç†é“¾
    augmented_data = []

    for idx, row in tqdm(samples_to_process.iterrows(), total=len(samples_to_process)):
        try:
            reasoning = generator.generate_reasoning_chain(
                question=row['question'],
                images=row['images'],
                ground_truth_answer=row['answer']
            )

            # æ„é€ å¢å¼ºåçš„è®­ç»ƒæ ·æœ¬
            augmented_sample = {
                'query': row['query'],
                'images': row['images'],
                'response': reasoning['reasoning_text'],  # åŒ…å«æ¨ç†è¿‡ç¨‹çš„å›ç­”
                'original_response': row['answer'],
                'has_reasoning': True
            }

            augmented_data.append(augmented_sample)

        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            continue

    # ä¿å­˜å¢å¼ºåçš„æ•°æ®
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in augmented_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"ç”Ÿæˆäº† {len(augmented_data)} ä¸ªå¸¦æ¨ç†é“¾çš„æ ·æœ¬")
    print(f"ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    batch_generate_reasoning_chains(
        train_data_path='train_b_dataset_for_image_0801.jsonl',
        output_path='train_b_dataset_with_reasoning.jsonl'
    )
```

#### æ­¥éª¤2: æ··åˆè®­ç»ƒæ•°æ®

```python
# æ–°å»ºæ–‡ä»¶: ccks2025_pdf_multimodal/round_b/merge_training_data.py

def merge_training_data(
    original_data_path,
    reasoning_data_path,
    output_path,
    reasoning_ratio=0.3
):
    """
    æ··åˆåŸå§‹æ•°æ®å’Œæ¨ç†é“¾æ•°æ®

    Args:
        reasoning_ratio: æ¨ç†é“¾æ•°æ®çš„æ¯”ä¾‹
    """
    # è¯»å–æ•°æ®
    original_data = []
    with open(original_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            original_data.append(json.loads(line))

    reasoning_data = []
    with open(reasoning_data_path, 'r', encoding='utf-8') as f:
        for line in f:
            reasoning_data.append(json.loads(line))

    # æŒ‰æ¯”ä¾‹æ··åˆ
    total_samples = len(original_data)
    reasoning_samples = int(total_samples * reasoning_ratio)

    # é‡‡æ ·
    import random
    random.seed(42)
    selected_reasoning = random.sample(reasoning_data, min(reasoning_samples, len(reasoning_data)))

    # åˆå¹¶
    merged_data = original_data + selected_reasoning

    # æ‰“ä¹±
    random.shuffle(merged_data)

    # ä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in merged_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"åˆå¹¶å®Œæˆ:")
    print(f"  åŸå§‹æ ·æœ¬: {len(original_data)}")
    print(f"  æ¨ç†é“¾æ ·æœ¬: {len(selected_reasoning)}")
    print(f"  æ€»æ ·æœ¬: {len(merged_data)}")
    print(f"  ä¿å­˜åˆ°: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    merge_training_data(
        original_data_path='train_b_dataset_for_image_0801.jsonl',
        reasoning_data_path='train_b_dataset_with_reasoning.jsonl',
        output_path='train_b_dataset_merged.jsonl'
    )
```

### ğŸ“Š é¢„æœŸæ•ˆæœ

**æ•°æ®å¢å¼º**:
- åŸè®­ç»ƒé›†: 1000æ ·æœ¬
- å¢å¼ºå: 1300æ ·æœ¬ (100%åŸå§‹ + 30%æ¨ç†é“¾)

**å‡†ç¡®ç‡æå‡**:
- é¢„è®¡æ•´ä½“å‡†ç¡®ç‡æå‡ **5-8%**
- ä½ç½®å…³ç³»é—®é¢˜å‡†ç¡®ç‡æå‡ **10-15%**

---

## æ–¹æ¡ˆ3: æ¨ç†æ—¶Chain-of-Thought

### ğŸ“‹ ç›®æ ‡
- æ¨ç†æ—¶è®©æ¨¡å‹è¾“å‡ºæ€è€ƒè¿‡ç¨‹
- ç‰¹åˆ«æ˜¯å¯¹äºä½ç½®å…³ç³»é—®é¢˜

### ğŸ’¡ æ ¸å¿ƒæ€è·¯
```
åŸæ–¹æ¡ˆ: é—®é¢˜ â†’ æ¨¡å‹ â†’ ç­”æ¡ˆ
æ–°æ–¹æ¡ˆ: é—®é¢˜ â†’ æ¨¡å‹ â†’ æ¨ç†æ­¥éª¤ â†’ ç­”æ¡ˆæå– â†’ æœ€ç»ˆç­”æ¡ˆ
```

### ğŸ”§ å…·ä½“å®æ–½æ­¥éª¤

#### æ­¥éª¤1: CoT Promptè®¾è®¡

```python
# ä¿®æ”¹ test_b_style_refer_215.py

def build_cot_prompt(question, images, question_type='position'):
    """
    æ„å»ºChain-of-Thoughtæç¤ºè¯

    Args:
        question_type: 'position' (ä½ç½®å…³ç³»), 'identification' (éƒ¨ä»¶è¯†åˆ«), 'other'
    """

    if question_type == 'position':
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“åˆ©åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æå›¾ç‰‡ï¼Œä¸€æ­¥æ­¥å›ç­”ä¸‹é¢çš„ä½ç½®å…³ç³»é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š

ã€æ­¥éª¤1ï¼šè¯†åˆ«å…³é”®éƒ¨ä»¶ã€‘
é¦–å…ˆï¼Œæˆ‘éœ€è¦åœ¨å›¾ä¸­æ‰¾åˆ°é—®é¢˜æåˆ°çš„éƒ¨ä»¶ï¼Œå¹¶è®°ä¸‹å®ƒä»¬çš„ç¼–å·ã€‚

ã€æ­¥éª¤2ï¼šè§‚å¯Ÿç©ºé—´ä½ç½®ã€‘
ç„¶åï¼Œæˆ‘éœ€è¦ä»”ç»†è§‚å¯Ÿè¿™äº›éƒ¨ä»¶åœ¨å›¾ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚

ã€æ­¥éª¤3ï¼šç¡®å®šæ–¹å‘å…³ç³»ã€‘
æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦ç¡®å®šå®ƒä»¬ä¹‹é—´çš„æ–¹å‘å…³ç³»ï¼ˆä¸Šä¸‹ã€å·¦å³ã€å‰åç­‰ï¼‰ã€‚

ã€æ­¥éª¤4ï¼šå¾—å‡ºç»“è®ºã€‘
æœ€åï¼ŒåŸºäºä»¥ä¸Šè§‚å¯Ÿï¼Œæˆ‘å¯ä»¥ç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚

ç°åœ¨è¯·å¼€å§‹ä½ çš„åˆ†æï¼Œå¹¶åœ¨æœ€åç”¨ã€æœ€ç»ˆç­”æ¡ˆã€‘æ ‡æ³¨ä½ çš„ç»“è®ºï¼š"""

    elif question_type == 'identification':
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“åˆ©åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æå›¾ç‰‡ï¼Œä¸€æ­¥æ­¥å›ç­”ä¸‹é¢çš„éƒ¨ä»¶è¯†åˆ«é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š

ã€æ­¥éª¤1ï¼šå®šä½ç›®æ ‡ç¼–å·ã€‘
é¦–å…ˆï¼Œæˆ‘éœ€è¦åœ¨å›¾ä¸­æ‰¾åˆ°é—®é¢˜æåˆ°çš„ç¼–å·ã€‚

ã€æ­¥éª¤2ï¼šè§‚å¯Ÿéƒ¨ä»¶ç‰¹å¾ã€‘
ç„¶åï¼Œæˆ‘éœ€è¦è§‚å¯Ÿè¿™ä¸ªç¼–å·æŒ‡å‘çš„éƒ¨ä»¶çš„å¤–è§‚å’Œç‰¹å¾ã€‚

ã€æ­¥éª¤3ï¼šç»“åˆä¸Šä¸‹æ–‡ã€‘
æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦ç»“åˆå›¾ç‰‡çš„æ•´ä½“ç»“æ„å’Œå…¶ä»–ä¿¡æ¯æ¥åˆ¤æ–­ã€‚

ã€æ­¥éª¤4ï¼šç»™å‡ºç­”æ¡ˆã€‘
æœ€åï¼Œæˆ‘å¯ä»¥ç¡®å®šè¿™ä¸ªéƒ¨ä»¶æ˜¯ä»€ä¹ˆã€‚

ç°åœ¨è¯·å¼€å§‹ä½ çš„åˆ†æï¼Œå¹¶åœ¨æœ€åç”¨ã€æœ€ç»ˆç­”æ¡ˆã€‘æ ‡æ³¨ä½ çš„ç»“è®ºï¼š"""

    else:
        # é»˜è®¤prompt
        prompt = question

    return prompt

def classify_question_type(question):
    """åˆ†ç±»é—®é¢˜ç±»å‹"""
    position_keywords = ['ä½ç½®', 'æ–¹å‘', 'ä¸Šæ–¹', 'ä¸‹æ–¹', 'å·¦ä¾§', 'å³ä¾§', 'å“ªé‡Œ', 'å“ªä¸ªä½ç½®']
    identification_keywords = ['æ˜¯ä»€ä¹ˆ', 'ä»€ä¹ˆéƒ¨ä»¶', 'å“ªä¸ªéƒ¨ä»¶', 'å«ä»€ä¹ˆ']

    if any(kw in question for kw in position_keywords):
        return 'position'
    elif any(kw in question for kw in identification_keywords):
        return 'identification'
    else:
        return 'other'
```

#### æ­¥éª¤2: ç­”æ¡ˆæå–

```python
def extract_final_answer(cot_response, style_examples=None):
    """
    ä»CoTå“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ

    Args:
        cot_response: åŒ…å«æ¨ç†è¿‡ç¨‹çš„å®Œæ•´å›ç­”
        style_examples: é£æ ¼ç¤ºä¾‹ï¼ˆç”¨äºè§„èŒƒåŒ–ç­”æ¡ˆæ ¼å¼ï¼‰

    Returns:
        ç®€æ´çš„æœ€ç»ˆç­”æ¡ˆ
    """
    import re

    # æ–¹æ³•1: æå–ã€æœ€ç»ˆç­”æ¡ˆã€‘æ ‡è®°çš„å†…å®¹
    answer_match = re.search(r'ã€æœ€ç»ˆç­”æ¡ˆã€‘\s*(.*?)(?:\n|$)', cot_response, re.DOTALL)
    if answer_match:
        raw_answer = answer_match.group(1).strip()
    else:
        # å¦‚æœæ²¡æœ‰æ ‡è®°ï¼Œä½¿ç”¨æœ€åä¸€å¥è¯
        sentences = cot_response.strip().split('ã€‚')
        raw_answer = sentences[-1] if sentences else cot_response

    # æ–¹æ³•2: ä½¿ç”¨å°æ¨¡å‹è¿›ä¸€æ­¥ç²¾ç‚¼ç­”æ¡ˆ
    if style_examples:
        refine_prompt = f"""è¯·å°†ä¸‹é¢çš„ç­”æ¡ˆç²¾ç‚¼ä¸ºç®€æ´çš„å½¢å¼ï¼ˆ20å­—ä»¥å†…ï¼‰ã€‚

å‚è€ƒé£æ ¼ç¤ºä¾‹ï¼š
{style_examples[0]}
{style_examples[1]}

å¾…ç²¾ç‚¼çš„ç­”æ¡ˆï¼š
{raw_answer}

ç²¾ç‚¼åçš„ç­”æ¡ˆï¼š"""

        # ä½¿ç”¨è½»é‡æ¨¡å‹å¿«é€Ÿæå–
        refined = origin_vllm([{"role": "user", "content": refine_prompt}], max_tokens=50)
        return refined.strip()

    return raw_answer.strip()
```

#### æ­¥éª¤3: é›†æˆåˆ°æ¨ç†æµç¨‹

```python
# ä¿®æ”¹ test_b_style_refer_215.py çš„ä¸»æ¨ç†å¾ªç¯

for idx in range(len(df_question)):
    question = df_question.loc[idx, 'question']
    document_name = df_question.loc[idx, 'document']

    # åˆ†ç±»é—®é¢˜
    question_type = classify_question_type(question)

    # è·å–ä¼˜åŒ–çš„å›¾åƒè¾“å…¥
    image_config = get_optimized_image_input(question, document_name, idx)

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨CoT
    use_cot = question_type in ['position', 'identification']

    if use_cot:
        # ä½¿ç”¨CoT prompt
        prompt = build_cot_prompt(question, image_config['images'], question_type)

        # è®¾ç½®æ›´é«˜çš„max_tokensä»¥å®¹çº³æ¨ç†è¿‡ç¨‹
        max_tokens = 1024
    else:
        # ä½¿ç”¨æ™®é€šprompt
        prompt = question
        max_tokens = 512

    # æ„é€ messages
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                *[{"type": "image", "image": img} for img in image_config['images']]
            ]
        }
    ]

    # ç”Ÿæˆå›ç­”ï¼ˆåŒ…å«æ¨ç†è¿‡ç¨‹ï¼‰
    cot_response = origin_vllm(messages, max_tokens=max_tokens)

    # æå–æœ€ç»ˆç­”æ¡ˆ
    if use_cot:
        # è·å–é£æ ¼ç¤ºä¾‹
        similar_q_idx = get_similar_question_embedding(idx, top_k=2)
        style_examples = get_options_for_similar_answer(similar_q_idx)

        final_answer = extract_final_answer(cot_response, style_examples)
    else:
        final_answer = cot_response

    # ä¿å­˜ç»“æœ
    result = {
        'question': question,
        'raw_response': cot_response if use_cot else None,  # å¯é€‰ï¼šä¿å­˜å®Œæ•´æ¨ç†è¿‡ç¨‹ç”¨äºåˆ†æ
        'answer': final_answer
    }

    results.append(result)
```

### ğŸ“Š é¢„æœŸæ•ˆæœ

**æ¨ç†è´¨é‡**:
- ä½ç½®å…³ç³»é—®é¢˜å‡†ç¡®ç‡æå‡ **10-15%**
- å¯è§£é‡Šæ€§æ˜¾è‘—æå‡

**æ€§èƒ½å½±å“**:
- æ¨ç†æ—¶é—´å¢åŠ çº¦ **30-50%** (å› ä¸ºmax_tokensæ›´å¤§)
- å¯é€šè¿‡ä»…å¯¹ç‰¹å®šé—®é¢˜ä½¿ç”¨CoTæ¥å¹³è¡¡

---

## ğŸ¯ ç»¼åˆå®æ–½æ–¹æ¡ˆ

### é˜¶æ®µ1: å¿«é€ŸéªŒè¯ (1å‘¨)

1. **å®æ–½æ–¹æ¡ˆ3** (CoT prompting)
   - æ— éœ€é¢å¤–æ•°æ®å‡†å¤‡
   - ç«‹å³å¯æµ‹è¯•æ•ˆæœ
   - é¢„æœŸæå‡: 8-12%

### é˜¶æ®µ2: æ•°æ®å¢å¼º (2å‘¨)

2. **å®æ–½æ–¹æ¡ˆ2** (æ¨ç†é“¾ç”Ÿæˆ)
   - ç”Ÿæˆæ¨ç†é“¾æ•°æ®
   - æ··åˆè®­ç»ƒ
   - é¢„æœŸæå‡: é¢å¤–5-8%

### é˜¶æ®µ3: æ·±åº¦ä¼˜åŒ– (3-4å‘¨)

3. **å®æ–½æ–¹æ¡ˆ1** (æ–‡æ¡£ç»“æ„åŒ–)
   - å¼€å‘ç‰ˆé¢åˆ†æå·¥å…·
   - ä¿®æ”¹é¢„å¤„ç†å’Œæ¨ç†æµç¨‹
   - é¢„æœŸæå‡: é¢å¤–10-15%

### ç´¯è®¡é¢„æœŸæ•ˆæœ

```
å½“å‰å‡†ç¡®ç‡: 82%
+ æ–¹æ¡ˆ3 (CoT): +10% â†’ 90.4%
+ æ–¹æ¡ˆ2 (æ¨ç†é“¾): +5% â†’ 94.9%
+ æ–¹æ¡ˆ1 (ç»“æ„åŒ–): +10% â†’ 100% (ç†è®ºä¸Šé™)
```

**å®é™…é¢„æœŸ**: æå‡è‡³ **90-95%** çš„å‡†ç¡®ç‡

---

## ğŸ“ å®æ–½æ£€æŸ¥æ¸…å•

### æ–¹æ¡ˆ1: æ–‡æ¡£ç»“æ„åŒ–
- [ ] å¼€å‘ DocumentStructureParser ç±»
- [ ] å®ç°å›¾ç‰‡è£å‰ªåŠŸèƒ½
- [ ] æ‰¹é‡è§£ææ‰€æœ‰æ–‡æ¡£
- [ ] ä¿®æ”¹é—®é¢˜è§£æé€»è¾‘
- [ ] æ›´æ–°æ¨ç†è„šæœ¬
- [ ] æµ‹è¯•å’ŒéªŒè¯

### æ–¹æ¡ˆ2: æ¨ç†é“¾ç”Ÿæˆ
- [ ] å¼€å‘ ReasoningChainGenerator
- [ ] æ‰¹é‡ç”Ÿæˆæ¨ç†é“¾
- [ ] åˆå¹¶è®­ç»ƒæ•°æ®
- [ ] é‡æ–°è®­ç»ƒæ¨¡å‹
- [ ] è¯„ä¼°æ•ˆæœ

### æ–¹æ¡ˆ3: CoTæ¨ç†
- [ ] è®¾è®¡CoT prompts
- [ ] å®ç°é—®é¢˜åˆ†ç±»
- [ ] å¼€å‘ç­”æ¡ˆæå–é€»è¾‘
- [ ] é›†æˆåˆ°æ¨ç†æµç¨‹
- [ ] æµ‹è¯•å’Œä¼˜åŒ–

---

## ğŸ”¬ å®éªŒå»ºè®®

### å¯¹æ¯”å®éªŒ

| å®éªŒç»„ | é…ç½® | é¢„æœŸå‡†ç¡®ç‡ |
|--------|------|------------|
| Baseline | å½“å‰æ–¹æ¡ˆ | 82% |
| Exp1 | Baseline + CoT | 90% |
| Exp2 | Baseline + æ¨ç†é“¾è®­ç»ƒ | 87% |
| Exp3 | Baseline + ç»“æ„åŒ– | 92% |
| Exp4 | Exp1 + Exp2 | 93% |
| Exp5 | Exp1 + Exp2 + Exp3 | 95% |

### è¯„ä¼°æŒ‡æ ‡

1. **æ•´ä½“å‡†ç¡®ç‡**: æ‰€æœ‰é—®é¢˜çš„å‡†ç¡®ç‡
2. **ä½ç½®å…³ç³»å‡†ç¡®ç‡**: ä¸“é—¨é’ˆå¯¹ä½ç½®å…³ç³»é—®é¢˜
3. **æ¨ç†æ—¶é—´**: å¹³å‡æ¯ä¸ªé—®é¢˜çš„æ¨ç†æ—¶é—´
4. **å¯è§£é‡Šæ€§**: äººå·¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„è´¨é‡

---

## ğŸ’¡ å…¶ä»–ä¼˜åŒ–å»ºè®®

### 1. å¤šæ¨¡å‹é›†æˆ
```python
# ä½¿ç”¨å¤šä¸ªcheckpointæŠ•ç¥¨
checkpoints = [
    'checkpoint-90',
    'checkpoint-180',
    'checkpoint-215'
]

# å¯¹æ¯ä¸ªé—®é¢˜ï¼Œå¤šä¸ªæ¨¡å‹åˆ†åˆ«æ¨ç†
answers = [model(question) for model in checkpoints]

# æŠ•ç¥¨æˆ–åŠ æƒèåˆ
final_answer = vote(answers)
```

### 2. ä¸»åŠ¨å­¦ä¹ 
```python
# å¯¹äºä½ç½®ä¿¡åº¦çš„é¢„æµ‹ï¼Œæ ‡æ³¨å¹¶åŠ å…¥è®­ç»ƒé›†
if confidence < 0.7:
    # äººå·¥æ ‡æ³¨æ­£ç¡®ç­”æ¡ˆ
    manual_label = get_human_annotation(question, prediction)
    # åŠ å…¥è®­ç»ƒé›†
    add_to_training_set(question, manual_label)
```

### 3. çŸ¥è¯†è’¸é¦
```python
# ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆteacherï¼‰è®­ç»ƒå°æ¨¡å‹ï¼ˆstudentï¼‰
teacher_model = Qwen2.5-VL-72B
student_model = Qwen2.5-VL-32B

# ç”¨teacherçš„è¾“å‡ºï¼ˆåŒ…æ‹¬æ¨ç†è¿‡ç¨‹ï¼‰è®­ç»ƒstudent
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **Chain-of-Thought Prompting**:
   - Wei et al. (2022): "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"

2. **æ–‡æ¡£ç‰ˆé¢åˆ†æ**:
   - LayoutLMv3, DocFormerç­‰æ–‡æ¡£ç†è§£æ¨¡å‹

3. **çŸ¥è¯†è’¸é¦**:
   - Hinton et al. (2015): "Distilling the Knowledge in a Neural Network"

4. **å¤šæ¨¡æ€æ¨ç†**:
   - Qwen-VLå®˜æ–¹æ–‡æ¡£å’Œæœ€ä½³å®è·µ

---

**æœ€åæ›´æ–°**: 2025-11-25
**çŠ¶æ€**: å¾…å®æ–½
**ä¼˜å…ˆçº§**: é«˜
